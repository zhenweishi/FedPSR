# architecture
arch: vit_base
enc_arch: MAEViTEncoder
dec_arch: MAEViTDecoder

# wandb
proj_name: mae3d
run_name: ${proj_name}_${arch}_${dataset}
wandb_id:
disable_wandb: False

# dataset
dataset: lung
#json_list: 'train.json'
data_path:
n_clients: 3
split_type: split_1


# output
output_dir:
log_dir :


# data preprocessing
space_x: 1
space_y: 1
space_z: 1
a_min: -1024.0
a_max: 3072.0
b_min: 0.0
b_max: 1.0
roi_x: 96
roi_y: 96
roi_z: 96
RandFlipd_prob: 0.2
RandRotate90d_prob: 0.2
RandScaleIntensityd_prob: 0.1
RandShiftIntensityd_prob: 0.1

# trainer
model_name : MAE3D
accum_iter : 1
clients_with_len : {}
dis_cvs_files : []
proxy_clients : []
clients_weightes : []
communication: 100
clip_grad : None
layer_decay: 0.65
E_epoch : 1
num_local_clients : -1
save_ckpt_freq : 500
trainer_name: MAE3DTrainer
batch_size: 8
vis_batch_size: 1
start_epoch: 0
warmup_epochs: 100
epochs: 2000
workers: 2
resume:

# model
patchembed: 'PatchEmbed3D'
pos_embed_type: 'sincos'
mask_ratio: 0.75
input_size: ${roi_x}
patch_size: 16
in_chans: 1
encoder_embed_dim: 768
encoder_depth: 12
encoder_num_heads: 12
decoder_embed_dim: 384
decoder_depth: 8
decoder_num_heads: 12

# optimizer
type: adamw
lr: 6.4e-3
beta1: 0.9
beta2: 0.95
weight_decay: 0.05

# logging
vis_freq: 100
save_freq: 500
print_freq: 1

# distributed processing
gpu: 0
dist_url:
world_size:
multiprocessing_distributed: False
dist_backend: nccl
distributed:
rank: 0
ngpus_per_node: 4
num_samples: 4

# randomness
seed:

# debugging
debug: false
